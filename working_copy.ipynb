{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneEMITUniversity: Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda \n",
    "\n",
    "- Introduction to Jupyter Notebook Environment\n",
    "- Introduction to Pandas\n",
    "- Introduction to Scikit-learn and Toy Datasets\n",
    "- Machine Learning Example: Iris Dataset (K nearest neighbors)\n",
    "- Machine Learning Example: Movie Review Classification (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\" Visit https://pandas.pydata.org/ for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a new dataframe of random integers between 0 and 10 with 8 rows and 5 columns\n",
    "#df = pd.DataFrame(np.random.randint(low=0, high=10, size=(8, 5)), columns=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "rocket = ['Rocket','Brown', 12,'Dog']\n",
    "mrbusiness = ['Mr. Business','Orange', 6, 'Cat']\n",
    "jerome = ['Jerome', 'White', 1000, 'Horse']\n",
    "nacho = ['Nacho', 'Gray', 100, 'Pterodactyl']\n",
    "lassie = ['Lassie','Brown', 20, 'Dog']\n",
    "piglet = ['Piglet', 'Pink', 5,'Pig']\n",
    "marvin = ['Marvin','Green',8, np.nan]\n",
    "\n",
    "df = pd.DataFrame([rocket, mrbusiness, jerome, nacho, lassie, piglet, marvin],columns=['Name','Color','Weight (lbs)','Species'])\n",
    "print('This is the original dataframe:')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates descriptive statistics that summarize the central tendency, \n",
    "# dispersion and shape of a dataset’s distribution, excluding NaN values.\n",
    "print('Descriptive statistical summary of the dataframe:')\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shape of the dataframe\n",
    "print('Shape of your dataframe: {}'.format(df.shape))\n",
    "print('There are {} rows.'.format(df.shape[0]))\n",
    "print('There are {} columns.'.format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first five rows\n",
    "print('These are the first three rows of the original dataframe:')\n",
    "display(df.head(3))\n",
    "\n",
    "# Look at the last five rows\n",
    "print('These are the last three rows of the original dataframe:')\n",
    "display(df.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a subset of your dataframe\n",
    "print('Grab only the rows that where color is brown.')\n",
    "display(df[df['Color'] == 'Brown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the values in a certain column\n",
    "print('How many of each value are there in the Species column?')\n",
    "display(df['Species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if any values are null\n",
    "print('Check how many null values are in each column')\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also read in information from csv, xlsx, txt files\n",
    "df = pd.read_excel('example_dataframe.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Scikit-learn and Toy Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, and k-means. Visit http://scikit-learn.org/stable/index.html for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've learned two ways so far to import your data: creating your own dataframe and reading in info from a file. You can also work with one of sklearn's toy datasets for a super quick start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\n",
    "- load_boston():\tLoad and return the boston house-prices dataset (regression).\n",
    "- load_iris():\tLoad and return the iris dataset (classification).\n",
    "- load_diabetes():\tLoad and return the diabetes dataset (regression).\n",
    "- load_digits():\tLoad and return the digits dataset (classification).\n",
    "- load_linnerud():\tLoad and return the linnerud dataset (multivariate regression).\n",
    "- load_wine():\tLoad and return the wine dataset (classification).\n",
    "- load_breast_cancer():\tLoad and return the breast cancer wisconsin dataset (classification).\n",
    "- load_sample_images():\tLoad sample images for image manipulation.\n",
    "\n",
    "These datasets are useful to quickly illustrate the behavior of the various algorithms implemented in the scikit. They are however often too small to be representative of real world machine learning tasks. Visit http://scikit-learn.org/stable/datasets/index.html for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with these datasets? Many types of analysis. Let's try classification first.\n",
    "## What is classification?\n",
    "One of the major areas of data science problems is classification. With classification algorithms, you take an existing dataset and use what you know about it to generate a predictive model for use in classification of future data points. If your goal is to use your dataset and its known subsets to build a model for predicting the categorization of future data points, you’ll want to use classification algorithms.\n",
    "\n",
    "When implementing supervised classification, you should already know your data’s subsets — these subsets are called categories. Classification helps you see how well your data fits into the dataset’s predefined categories so that you can then build a predictive model for use in classifying future data points.\n",
    "\n",
    "\n",
    "To classify things, you look at the different characteristics of something. Characteristics can be things like height, length, etc. \n",
    "\n",
    "Classification is the task of choosing the correct class label for a given input. In basic classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. Some examples of classification tasks are:\n",
    "\n",
    "Deciding whether an email is spam or not.\n",
    "Deciding what the topic of a news article is, from a fixed list of topic areas such as \"sports,\" \"technology,\" and \"politics.\"\n",
    "Deciding whether a given occurrence of the word bank is used to refer to a river bank, a financial institution, the act of tilting to the side, or the act of depositing something in a financial institution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example follows https://github.com/amueller/introduction_to_ml_with_python/blob/master/01-introduction.ipynb.\n",
    "\n",
    "Let's perform a classification problem using one of the toy datasets. Let's see if we can identify what species a random iris is  based on only a few of their physical characteristics. Namely, the length and width of their petal and of their sepal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flower Education Break for the non-botanists in the room:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.wpclipart.com/plants/diagrams/plant_parts/petal_sepal_label.png',width=200,height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris data set contains information abotu 150 different irises. There are 3 different species with 50 examples of each.\n",
    "\n",
    "What we want to predict: species of iris plant. \n",
    "\n",
    "Information we know about each plant: \n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "\n",
    "#### Based on this, can we predict the classification of other iris flowers? Let's find out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's contained in the iris_dataset?\n",
      "Keys of iris_dataset: dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "# First let's load the dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "# These datasets have a  little bit of a unique format. Let's explore it briefly.\n",
    "print(\"What's contained in the iris_dataset?\")\n",
    "print(\"Keys of iris_dataset: {}\".format(iris_dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find out the different types of each piece of information.\n",
    "print(\"Type of 'data' data: {}\".format(type(iris_dataset['data'])))\n",
    "print(\"Type of 'target' data: {}\".format(type(iris_dataset['target'])))\n",
    "print(\"Type of 'target_names' data: {}\".format(type(iris_dataset['target_names'])))\n",
    "print(\"Type of 'DESCR' data: {}\".format(type(iris_dataset['DESCR'])))\n",
    "print(\"Type of 'feature_names' data: {}\".format(type(iris_dataset['feature_names'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check the Iris Dataset Description for detailed info: \\n')\n",
    "print(iris_dataset['DESCR'][:970] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"raw data\" contains the information about the length and width of the petal and sepal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does the raw data look like? \n",
      "\n",
      "The shape of raw data is: (150, 4)\n",
      "First five rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "What does each column refer to? Check the feature_names.\n",
      "Feature names:\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# View the raw data.\n",
    "print('What does the raw data look like? \\n')\n",
    "print(\"The shape of raw data is: {}\".format(iris_dataset['data'].shape))\n",
    "print(\"First five rows of data:\\n{}\\n\".format(iris_dataset['data'][:5]))\n",
    "\n",
    "# Find out the feature names.\n",
    "print(\"What does each column refer to? Check the feature_names.\")\n",
    "print(\"Feature names:\\n{}\".format(iris_dataset['feature_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"target data\" contains the information about the species of the irises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does the target data look like?\n",
      "\n",
      "The Shape of the target data: (150,)\n",
      "The Target data:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Each row corresponds to one flower. Each flower has a type and so there is one target label for each row of data. \n",
      "\n",
      "What are the target names? ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# View the target data.\n",
    "print(\"What does the target data look like?\\n\")\n",
    "print(\"The Shape of the target data: {}\".format(iris_dataset['target'].shape))\n",
    "print(\"The Target data:\\n{}\\n\".format(iris_dataset['target']))\n",
    "\n",
    "# Find out the target names.\n",
    "print(\"Each row corresponds to one flower. Each flower has a type and so there is one target label for each row of data. \\n\")\n",
    "print(\"What are the target names? {}\".format(iris_dataset['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the iris data in a pandas dataframe for easy data manipulation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from the iris data\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "iris_dataframe = pd.DataFrame(iris_dataset['data'], columns=iris_dataset.feature_names)\n",
    "display(iris_dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize our data to start to increase our understanding of what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot Sepal Length vs Petal Length\n",
    "plt.scatter(iris_dataframe['sepal length (cm)'], iris_dataframe['petal length (cm)'], c=iris_dataset['target'])\n",
    "plt.title(\"Petal Length vs Sepal Length\")\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('petal length (cm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sepal Width vs Petal Width\n",
    "plt.scatter(iris_dataframe['sepal width (cm)'], iris_dataframe['petal width (cm)'], c=iris_dataset['target'])\n",
    "plt.title(\"Petal Width vs Sepal Width\")\n",
    "plt.xlabel('sepal width (cm)')\n",
    "plt.ylabel('petal width (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Example: The Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get down to business and see if we can build a classification model that can predicts the species of an iris based on its petal/sepal length/width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set / Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way for us to get a handle on the ability of a predictive model to perform on future data is to try to simulate this eventuality. Although we cannot literally gain access to the future before it occurs we can reserve some of our currently available data and treat it as if were data from the future. \n",
    "\n",
    "The simplest partition possible for cross-sectional data is a two-way random partition to generate a learning (or training) set and a test set (sometimes instead referred to as a validation set). The thinking underlying such a division is that:\n",
    "\n",
    "- The data available for analytics fairly represents the real world processes we wish to model\n",
    "- The real world processes we wish to model are expected to remain relatively stable over time so that a well-constructed model built on last month’s data is reasonably expected to perform adequately on next month’s data \n",
    "\n",
    "If our assumptions are more or less correct then the data we have today is a reasonable representation of the data we expect to have in the future. Holding back some of today’s data for testing is therefore a fair approximation to having future data for testing.  \n",
    "\n",
    "The learn partition has a single and essential role: it provides the raw material from which the predictive model is generated. \n",
    "\n",
    "Among other uses, the test partition is employed to evaluate the performance of the model. Any given sized tree built on the learn data commits to specific predictions which can be compared to test data actual outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "\n",
    "# X refers to our iris data\n",
    "# y refers to our target data\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is knn? \n",
    "This example follows from https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/.\n",
    "\n",
    "In this article, we will talk about another widely used classification technique called K-nearest neighbors (KNN) . Our focus will be primarily on how does the algorithm work and how does the input parameter effect the output/prediction. KNN can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry. \n",
    "\n",
    "Let’s take a simple case to understand this algorithm. Following is a spread of red circles (RC) and green squares (GS) :\n",
    "\n",
    "Insert picture here.\n",
    "\n",
    "You intend to find out the class of the blue star (BS) . BS can either be RC or GS and nothing else. The “K” is KNN algorithm is the nearest neighbors we wish to take vote from. Let’s say K = 3. Hence, we will now make a circle with BS as center just as big as to enclose only three datapoints on the plane. Refer to following diagram for more details:\n",
    "\n",
    "Insert next picture here.\n",
    "\n",
    "The three closest points to BS is all RC. Hence, with good confidence level we can say that the BS should belong to the class RC. Here, the choice became very obvious as all three votes from the closest neighbor went to RC. The choice of the parameter K is very crucial in this algorithm. Next we will understand what are the factors to be considered to conclude the best K.\n",
    "\n",
    "How do we choose the factor K?\n",
    "\n",
    "We can implement a KNN model by following the below steps:\n",
    "\n",
    "1. Load the data\n",
    "2. Initialise the value of k\n",
    "3. For getting the predicted class, iterate from 1 to total number of training data points\n",
    "    1. Calculate the distance between test data and each row of training data. Here we will use Euclidean distance as our distance metric since it’s the most popular method. The other metrics that can be used are Chebyshev, cosine, etc.\n",
    "    2. Sort the calculated distances in ascending order based on distance values\n",
    "    3. Get top k rows from the sorted array\n",
    "    4. Get the most frequent class of these rows\n",
    "    5. Return the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lucky for us we don't have to implement the KNN algorithm from scratch. \n",
    "# These libraries have already done this for us!\n",
    "# Let's import it.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initalize a KNN classifier for k=2\n",
    "k = 2\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Fit the model using X as training data and y as target values\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what it  looks like to make a prediction on one random example. Create a new random example of an iris flower and see if we can predict what species it would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new.shape: (1, 4)\n",
      "X_new: [[5.  2.9 1.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "# Create a new example.\n",
    "# This example would have a sepal length of 5 cm, sepal width of 2.9 cm, petal length of 1 cm, and petal width of 0.2 cm.\n",
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "print(\"X_new.shape: {}\".format(X_new.shape))\n",
    "print(\"X_new:\", X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict a response for our new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "query data dimension must match training data dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-eb6375f1763a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# predict the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction Class: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"Predicted target name: {}\".format(\n\u001b[1;32m      5\u001b[0m        iris_dataset['target_names'][prediction]))\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    384\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/neighbors/binary_tree.pxi\u001b[0m in \u001b[0;36msklearn.neighbors.kd_tree.BinaryTree.query\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: query data dimension must match training data dimension"
     ]
    }
   ],
   "source": [
    "# predict the response\n",
    "prediction = knn.predict(X_new)\n",
    "print(\"Prediction Class: {}\".format(prediction))\n",
    "print(\"Predicted target name: {}\".format(\n",
    "       iris_dataset['target_names'][prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now make predictions on our entire test set and evaluate our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 test examples.\n",
      "The test set predictions:\n",
      " ['Martian' 'Dog' 'Dog']\n",
      "The actual test set target values are:\n",
      "6     Martian\n",
      "11        Dog\n",
      "4         Dog\n",
      "Name: Species, dtype: object\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(\"We have {} test examples.\".format(X_test.shape[0]))\n",
    "print(\"The test set predictions:\\n {}\".format(y_pred))\n",
    "print(\"The actual test set target values are:\\n{}\".format(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How many of the predictions are the same as the actual values?? How many are different??\")\n",
    "print(y_pred == y_test)\n",
    "\n",
    "num_diff = len(y_pred) - (y_pred == y_test).sum()\n",
    "\n",
    "print('You can see above that {} prediction(s) is/are incorrect.'.format(num_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use knn.score to evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the mean accuracy on the given test data and labels.\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test example (Delete below for final version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marvin = ['Marvin','Green',10, 'Martian']\n",
    "martian2 = ['Marvin','Green',9, 'Martian']\n",
    "piglet2 = ['Marvin','Pink',8, 'Pig']\n",
    "martian3 = ['Marvin','Green',9, 'Martian']\n",
    "piglet3 = ['Marvin','Pink',8, 'Pig']\n",
    "dog2 = ['Marvin','Brown',15, 'Dog']\n",
    "\n",
    "animals = [rocket, mrbusiness, jerome, nacho, lassie, piglet, marvin, martian2, piglet2, martian3, piglet3, dog2]\n",
    "df = pd.DataFrame(animals,columns=['Name','Color','Weight (lbs)','Species'])\n",
    "s1 = pd.get_dummies(df['Color'])\n",
    "df = pd.concat([df.drop('Color',axis=1), s1], axis=1)\n",
    "\n",
    "print('This is the original dataframe:')\n",
    "display(df)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Name','Species'],axis=1), df['Species'], random_state=0)\n",
    "display(X_train)\n",
    "display(X_test)\n",
    "\n",
    "# Initalize a KNN classifier for k=2\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "# Fit the model using X as training data and y as target values\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "X_test = pd.read_csv('somefile.csv')\n",
    "\n",
    "# Evaluate (returns the mean accuracy on the given test data and labels)\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "print('Prediction')\n",
    "print(y_pred)\n",
    "print('Actuals')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Example: Movie Review Classification (NLP)\n",
    "\n",
    "This example leverages the Document Classification activity from Chapter 6 of Natural Language Processing with Python: https://www.nltk.org/book/ch06.html. There are many other exercises in the book we recommend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As with sklearn, nltk also provides some example datasets in its corpus.  One such example is a list of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movie_reviews.\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store our movie reviews in a list. Each element of the list will store information about one review. We will have the review itself and also its corresponding category (either positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples where each tuple contains (the movie review, its corresponding category)\n",
    "documents = [(list(movie_reviews.words(fileid)), category) \\\n",
    "             for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the first two examples of our reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example review 1\n",
    "print(\"Let's look at our first review.\")\n",
    "print('The classification of this review is: {}'.format(documents[1][1]))\n",
    "print('The Review Text:\\n{}...'.format(\" \".join(documents[1][0][:100])))\n",
    "print('--------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "# Example review 2\n",
    "print(\"Let's look at our second review.\")\n",
    "print('The classification of this review is: {}'.format(documents[2][1]))\n",
    "print('The Review Text:\\n{}...'.format(\" \".join(documents[2][0][:100])))\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea here is to look at which words are contained in the review text and see if we can use that information to help us decide which class the review belongs to. For example, maybe if the review contains the word \"amazing\" 300 times, then the review is positive. What are the steps we need to take to accomplish this?\n",
    "    - Make a list of all the words in the entire corpus. \n",
    "    - Count how many times each word appears in the corpus.\n",
    "    - Sort the list from most frequently used word to least frequently used.\n",
    "    - Select the top 2000 words.\n",
    "    - For each document, count how many times each of these 2000 words appears.\n",
    "        - Right now, each review is a series of sentences that we parse into meaningful messages as we read them. The machine cannot do this. It can't read in the same sense that we can. Instead, we'll get the machine to represent each document as a list of the words contained in the review as well as the frequency of each word. \n",
    "    - Separate the reviews into test set and train set.\n",
    "    - Train/fit a Naive Bayes Classifier.\n",
    "    - Test the model's accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.shuffle(documents)\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Create a frequency dictionary with all the words \n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "\n",
    "# Take the top 2000 most frequent words.\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "# Function to determine words in one specific review.\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Create the review representations for each review.\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier based on the Naive Bayes algorithm.  In order to find the\n",
    "probability for a label, this algorithm first uses the Bayes rule to\n",
    "express P(label|features) in terms of P(label) and P(features|label):\n",
    "\n",
    "\\begin{equation*}\n",
    "P(\\text{label | features}) = \\frac{\\text{P(label)}*\\text{P(features | label)}}{\\text{P(features)}}\n",
    "\\end{equation*}\n",
    "\n",
    "The algorithm then makes the 'naive' assumption that all features are\n",
    "independent, given the label:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(\\text{label | features}) = \\frac{\\text{P(label)}*P(f_1\\text{| label)}*...*P(f_n\\text{| label})}{\\text{P(features)}}\n",
    "\\end{equation*}\n",
    "\n",
    "Rather than computing P(features) explicitly, the algorithm just\n",
    "calculates the numerator for each label, and normalizes them so they\n",
    "sum to one:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(\\text{label | features}) = \\frac{\\text{P(label)}*P(f_1\\text{| label)}*...*P(f_n\\text{| label})}\n",
    "{\\sum_{l} P(l)*P(f_1|l)*...*P(f_n|l)}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the reviews into a training set and a test set.\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "\n",
    "# Initalize and train our Naive Bayes Classifier. (Documentation at https://www.nltk.org/_modules/nltk/classify/naivebayes.html)\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate the accuracy of the classifer on the test set.\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at one example:\n",
    "review_features, classification = test_set[0]\n",
    "\n",
    "print('The classification of this example review is: {}.'.format(classification))\n",
    "for feature in list(review_features.keys())[:10]:\n",
    "    print(feature, review_features[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show results on one test example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this tells you is the ratio of occurences in negative to positive, or visa versa, for every word. So here, we can see that the term \"recognizes\" appears 8.1 more times as often in positive reviews as it does in negative reviews. \"Unimaginative\" appears 7.8 more times as often in negative reviews as it does in positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the most relevant features, and display them.\n",
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
